run_id: proposed-iter1-Qwen3-0.6B-0.6-B-params-gsm8k
method: KDLS-QLoRA
model:
  name: Qwen/Qwen3-0.6B
  architecture: causal-transformer
  quantization:
    scheme: nf4
    bits: 4
  adapter:
    type: lora
    rank: 64
    alpha: 32
    dropout: 0.05
  kdls:
    enabled: true
    beta: 0.95          # EMA for X̄ and Ḡ
    polyak_ema: 0.7     # smoothing for step scale
    polyak_max_scale: 2.0
    epsilon: 1.0e-12
    fallback_clip: false
    buffers_per_adapter: 2       # X̄ and Ḡ
    buffer_size: 64              # r × r

dataset:
  name: gsm8k
  config: main
  splits:
    train: train
    validation: test             # GSM8K has no official dev; use test as dev here
  text_max_length: 512
  preprocessing:
    strip_whitespace: true
    add_eos_token: true

training:
  batch_size: 32                 # effective (micro=4, grad_acc=8)
  micro_batch_size: 4
  gradient_accumulation_steps: 8
  epochs: 3
  max_steps: null                # let epochs drive stopping
  precision: bf16
  optimizer:
    type: adamw
    betas: [0.9, 0.999]
    weight_decay: 0.0
    learning_rate: 1.0           # Adam as pre-conditioner only, scale decided by KDLS
  lr_scheduler: null             # not used – step size from KDLS
  clipping:
    max_grad_norm: 1.0
  seed: 42
  log_every_steps: 10
  save_every_epochs: 1

optuna:
  n_trials: 25
  direction: maximize            # GSM8K accuracy
  search_space:
    kdls.beta:
      type: uniform
      low: 0.90
      high: 0.99
    kdls.polyak_max_scale:
      type: uniform
      low: 1.0
      high: 2.0
    adapter.rank:
      type: categorical
      choices: [32, 64]
